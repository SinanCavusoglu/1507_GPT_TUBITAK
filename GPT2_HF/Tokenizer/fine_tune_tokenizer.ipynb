{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import (Sequence, NFD)\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_law_par = pd.read_parquet(r\"c:\\Users\\siren\\Desktop\\1507\\dataset\\hukuk_raw_dataset\\train.parquet\", engine=\"pyarrow\")\n",
    "wiki_law_par = pd.read_parquet(r\"C:\\Users\\siren\\Desktop\\1507\\dataset\\ts_wikipedia\\bpe_support.parquet\")\n",
    "concat_par = pd.concat([train_law_par, wiki_law_par]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kötü kadın tiplemesi derken Bellerophontes efs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vatandaşlık kavramına yeni bir yaklaşım getiri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALİ YEŞİLIRMAKIN TEBLİĞİ Arabuluculuk şartı ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deniz İş Kanununda özel bir düzenleme bulunmay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.3. Sosyal Hukuk Devleti lkesi Anlayıına Duyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372004</th>\n",
       "      <td>Ancak bipolar koordinatlarda bu ifade burada a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372005</th>\n",
       "      <td>Bipolar koordinatların klasik uygulamaları kıs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372006</th>\n",
       "      <td>Tipik bir örnek olarak iki paralel silindirik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372007</th>\n",
       "      <td>Bipolar üç boyutlu Ortogonal koordinatların bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372008</th>\n",
       "      <td>Bipolar silindirik koordinatlarda \" z \" yönüne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5396536 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        Kötü kadın tiplemesi derken Bellerophontes efs...\n",
       "1        vatandaşlık kavramına yeni bir yaklaşım getiri...\n",
       "2        ALİ YEŞİLIRMAKIN TEBLİĞİ Arabuluculuk şartı ve...\n",
       "3        Deniz İş Kanununda özel bir düzenleme bulunmay...\n",
       "4        3.3. Sosyal Hukuk Devleti lkesi Anlayıına Duyu...\n",
       "...                                                    ...\n",
       "2372004  Ancak bipolar koordinatlarda bu ifade burada a...\n",
       "2372005  Bipolar koordinatların klasik uygulamaları kıs...\n",
       "2372006  Tipik bir örnek olarak iki paralel silindirik ...\n",
       "2372007  Bipolar üç boyutlu Ortogonal koordinatların bi...\n",
       "2372008  Bipolar silindirik koordinatlarda \" z \" yönüne...\n",
       "\n",
       "[5396536 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_par = concat_par.iloc[:-1]\n",
    "concat_par.drop(columns=[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_par.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \" \"]\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = Sequence([NFD()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(vocab_size=52000,\n",
    "                     min_frequency = 1,\n",
    "                     special_tokens=special_tokens,\n",
    "                     max_token_length = 6,\n",
    "                     show_progress = True)\n",
    "tokenizer.train_from_iterator(concat_par[\"text\"].astype(str), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model.save('.')\n",
    "tokenizer.save(\"MyBPETokenizerWikiLaw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['Bırak', 'ın', ' ', 'ada', 'let', ' ', 'yer', 'ini', ' ', 'bul', 'sun', ',', ' ', 'ister', 'se', ' ', 'kıya', 'met', ' ', 'kop', 'sun', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizerFromFile = Tokenizer.from_file(\"MyBPETokenizerWikiLaw.json\")\n",
    "sen_enc3 = tokenizerFromFile.encode(\"Bırakın adalet yerini bulsun, isterse kıyamet kopsun.\")\n",
    "print(f\"Output: {format(sen_enc3.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['Ada', 'let', ' ', 'önce', ' ', 'dev', 'let', 'ten', ' ', 'gelir', '.', ' ', 'Çün', 'kü', ' ', 'hukuk', ',', ' ', 'dev', 'letin', ' ', 'top', 'lum', 'sal', ' ', 'düz', 'eni', 'dir', '.']\n"
     ]
    }
   ],
   "source": [
    "sen_enc3 = tokenizerFromFile.encode(\"Adalet önce devletten gelir. Çünkü hukuk, devletin toplumsal düzenidir.\")\n",
    "print(f\"Output: {format(sen_enc3.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=r\"C:\\Users\\siren\\Desktop\\1507\\GPT2_HF\\Tokenizer\\MyBPETokenizerWikiLaw.json\",\n",
    "    merges_file=r\"C:\\Users\\siren\\Desktop\\1507\\GPT2_HF\\Tokenizer\\merges.txt\",\n",
    "    vocab_file=r\"C:\\Users\\siren\\Desktop\\1507\\GPT2_HF\\Tokenizer\\vocab.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adalet önce devletten gelir. Çünkü hukuk, devletin toplumsal düzenidir.\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"Adalet önce devletten gelir. Çünkü hukuk, devletin toplumsal düzenidir.\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723b2bc7d343423c9e18febd34dce991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdce043a61084b14aa2f47630646525d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sergeantson/1507_Law_Tokenizer/commit/dff3a162f66fdd7db17c7c2625e72b8514c46c19', commit_message='Upload tokenizer', commit_description='', oid='dff3a162f66fdd7db17c7c2625e72b8514c46c19', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"sergeantson/1507_Law_Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90226d90d8df444480ab83c43bfd967d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3420cd511da04e79877a1b470ad6fae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7328b130341a49e119eac2500ce09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0414964af0b44ea0a9ef3132ef6bc446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ada', '##let', 'M', '##ü', '##lk', '##ü', '##n', 'Te', '##mel', '##id', '##ir']\n"
     ]
    }
   ],
   "source": [
    "push_tokenizer = AutoTokenizer.from_pretrained(\"sergeantson/1507_Law_Tokenizer\")\n",
    "sequence = \"Adalet Mülkün Temelidir\"\n",
    "tokens = push_tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19222, 5765, 150, 17176, 10493, 17176, 1179, 12008, 10212, 2386, 3161]\n"
     ]
    }
   ],
   "source": [
    "ids = push_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adalet Mülkün Temelidir\n"
     ]
    }
   ],
   "source": [
    "decoded_string = push_tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tubitak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
