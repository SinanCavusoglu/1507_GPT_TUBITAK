{
    "output_dir": "./results",
    "tokenizer": "sergeantson/1507_Law_Tokenizer",
    "dataset_path": "./tokenized_law",
    "size": "gpt2",  
    "batch_size": 64,
    "block_size": 1024,
    "gradient_accumulation": 1,
    "eval_interval": 1000,
    "log_interval": 10,
    "warmup_iters": 3000,
    "max_iters": 600000,
    "weight_decay": 0.1,
    "decay_lr": true,
    "grad_clip": 1.0,
    "learning_rate": 5e-5,
    "wandb" : false
  }